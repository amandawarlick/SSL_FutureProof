---
title: "SSL Future Proof Project Update - Jan 2023"
output: html_document
---

```{r setup, include=FALSE, fig.align = 'center'}

knitr::opts_chunk$set(echo = FALSE, eval = T, message = F, warning = F, fig.align = 'center')

library(here) #v1.0.1
library(dplyr) #v1.0.5
library(nimble) #v0.12.1
library(coda)
library(ggh4x)
library(readr)
library(knitr)
library(Hmisc)
library(stringr)

source(here::here('scripts', 'PlotTheme.R'))

```

### Project goal    
The goal of this project was to examine model performance in the estimation of Steller sea lion survival rates across a range of mark-recapture survey designs. We have thought of this in two contexts: maintaining an accurate/unbiased understanding of a mean survival estimate and being able to detect a trend or change in survival from a "baseline" status quo mean rate.  

### Simulation analysis  

#### Model framework  
Mark-recapture data were simulated over 10- and 20-year study horizons based on empirical age-specific survival and detection probabilities estimated for the eastern DPS. These data were fit to a Cormack Jolly Seber mark-recapture model framework to estimate survival of five age classes (pups, age-1, age-2, juveniles (ages 3-4), and adults 5+). Temporal variance was included (based on empirical estimates from the eDPS) for age class-specific survival probabilities, but detection probabilities were assumed constant over the study period. Uninformative priors were used for the estimation of mean intercepts for detection and survival probabilities. Penalized complexity priors with a fixed shrinkage rate were used to estimate temporal variance in survival probabilities. More details about data generation, model framework, and Bayesian implementation will be included in the formal write-up. 

#### Scenarios

The above model was fit using simulated data across 45 scenarios with varying branding cohort sizes (N = 50, 75, 100, 150, 200), branding frequency (annual, biennial, triennial), and detection rates (low, medium, high). Low and high detection rates were set at +/- 20% of the medium level (based on empirical estimates; e.g., simulated detection rate levels were 0.48, 0.6, 0.72 for pups). Resighting effort was assumed to occur annually regardless of branding schedule based on discussions about the likely future reliance on remote cameras. Therefore, this could be seen as a "best case" scenario with resighting effort occurring each year. 

#### Model performance  

Model performance was examined using relative bias, RMSE, coefficient of variation (CV), and the probability of detecting a change in survival over both 10- and 20-year study horizons. The 20-year study horizon represents a situation where you have a population that already includes previously marked individuals (as is the case for Stellers) rather than starting a new 10-year study. 

To examine the ability to detect a change in survival from a "baseline", I generated annual survival estimates using a logit normal distribution with a mean rate derived by multiplying the empirical mean by an age-based change threshold using the standard deviation of empirical eDPS estimates. I then examined the probability of whether the credible intervals for each survival rate excluded the original "baseline" rate at 5, 10, 15, and 20 years into the study. I examined the ability to detect a "small" decrease (15%, 10%, 5%, 2.5%) and a "big" decrease (20%, 15%, 10%, 5%) in survival for pups, age-1:2, juveniles, and adults, respectively.

The proportion of simulations where all parameters converged (Rhat < 1.1) was high (~95%), though convergence was a bit lower for some scenarios. 

#### Caveats  
(1) This framework is estimating mean survival rates per age class, so there would be higher relative uncertainty for sex-specific estimates with these sample sizes. 
(2) This does not include improvements in precision that could be gained using an integrated population model framework that would capitalize on information from aerial survey data or other rookery-based counts. 

<!-- Table showing the scenario features (branding frequency, detection probability, cohort branding size (N)), the number of simulations per scenario (tot), and the proportion of those simulations where all parameters converged (Rhat < 1.1). Simulations where not all parameters converged were filtered out of the below results.   -->


```{r convergence summary 10}

sc <- c(1:45)

scenarios <- data.frame(scenario = c(sc),
                        survey_freq = c(rep(1,15), rep(2,15), rep(3,15)),
                        detection = c(rep(c('Low', 'Low', 'Low', 'Low', 'Low',
                                            'Med', 'Med', 'Med', 'Med', 'Med', 
                                            'High', 'High', 'High', 'High', 'High'), 3)),
                        N = c(rep(c(50,75,100,150,200), 9)))
                        
med.files <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'median', all.files = FALSE, full.names = F)


converged <- data.frame()
for (f in 1:length(med.files)) {

rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge','sims', 
                                                     med.files[f]))[,'max_rhat']) %>%
  transform(scenario = parse_number(gsub('S.', '', med.files[f]))) %>%
  transform(batch = str_sub(med.files[f], -6,-5)) %>%
  transform(rhat = as.numeric(rhat))

converged <- rbind(converged, rhats)

}

tot_runs <- converged %>%
  filter(!is.na(rhat)) %>%
  # filter(batch %in% c('n9', 10,11,12,13)) %>%
  transform(type = ifelse(batch %in% c('n7', 'n9', 11, 13), 'Small change', 'Big change')) %>%
  group_by(scenario, type) %>%
  dplyr::summarize(tot = n())

prop_conv <- converged %>%
  filter(rhat<1.1) %>%
  transform(type = ifelse(batch %in% c('n7', 'n9', 11, 13), 'Small change', 'Big change')) %>%
  group_by(scenario, type) %>%
  dplyr::summarize(conv = n(), .groups = 'keep') %>%
  merge(tot_runs, by = c('scenario', 'type'), all = T) %>%
  transform(prop_conv = round(conv/tot,2)) %>%
  dplyr::select(scenario, type, tot, prop_conv)

scenarios_tab <- scenarios %>%
  merge(prop_conv, by = 'scenario', all = T) %>%
    transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_freq = factor(survey_freq, levels = c(1,2,3),
                                 labels = c('Annual', 'Biennial', 'Triennial')))

missing <- scenarios_tab %>%
  filter(tot<100)

# kable(scenarios_tab)

# ggplot(scenarios_tab %>% filter(!is.na(prop_conv)), aes(x = factor(detection), y = factor(N), fill = prop_conv)) +
#   geom_tile(color = 'grey50') +
#   xlab('Detection') +
#   ylab('Sample size') +
#   facet_nested(type ~ survey_freq, drop = T,
#              # labeller = labeller(survey_freq = freq_labels),
#              scales = 'free_x') +
#   scale_fill_gradient2(name = "Convergence",
#                       mid = "#FFFFFF", high = "deepskyblue1", low = "red2", midpoint = 0.95) +
#   theme_bw() +
#   theme(legend.position = 'right',
#              strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
#         legend.text = element_text(size = 8, vjust = 0.5),
#         legend.title = element_text(vjust = 0.8, size = 10)) 


```


```{r convergence summary 20}

sc <- c(1:45)

scenarios <- data.frame(scenario = c(sc),
                        survey_freq = c(rep(1,15), rep(2,15), rep(3,15)),
                        detection = c(rep(c('Low', 'Low', 'Low', 'Low', 'Low',
                                            'Med', 'Med', 'Med', 'Med', 'Med', 
                                            'High', 'High', 'High', 'High', 'High'), 3)),
                        N = c(rep(c(50,75,100,150,200), 9)))
                        
med.files20 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'median', all.files = FALSE, full.names = F)


converged20 <- data.frame()
for (f in 1:length(med.files20)) {

rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge','sims', 'prePop',
                                                     med.files20[f]))[,'max_rhat']) %>%
  transform(scenario = parse_number(gsub('S.', '', med.files20[f]))) %>%
  transform(batch = str_sub(med.files20[f], -6,-5)) %>%
  transform(rhat = as.numeric(rhat))

converged20 <- rbind(converged20, rhats)

}

tot_runs <- converged20 %>%
  filter(!is.na(rhat)) %>%
  transform(type = ifelse(batch %in% c(seq(10,31, by = 2)), 'Small change', 'Big change')) %>%
  group_by(scenario, type) %>%
  dplyr::summarize(tot = n(), .groups = 'keep')

prop_conv20 <- converged20 %>%
  filter(rhat<1.1) %>%
  transform(type = ifelse(batch %in% c(seq(10,31, by = 2)), 'Small change', 'Big change')) %>%
  group_by(scenario, type) %>%
  dplyr::summarize(conv = n(), .groups = 'keep') %>%
  merge(tot_runs, by = c('scenario', 'type'), all = T) %>%
  transform(prop_conv = round(conv/tot,2)) %>%
  dplyr::select(scenario, type, tot, prop_conv)


scenarios_tab20 <- scenarios %>%
  merge(prop_conv20, by = 'scenario', all = T) %>%
    transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%

  transform(survey_freq = factor(survey_freq, levels = c(1,2,3),
                                 labels = c('Annual', 'Biennial', 'Triennial')))

missing20 <- converged20 %>%
  filter(!is.na(rhat)) %>%
  transform(type = ifelse(batch %in% c(seq(10,31, by = 2)), 'Small change', 'Big change')) %>%
  group_by(scenario, batch, type) %>%
  dplyr::summarize(tot = n(), .groups = 'keep')

# kable(scenarios_tab20)

# ggplot(scenarios_tab20 %>% filter(!is.na(prop_conv)), aes(x = factor(detection), y = factor(N), fill = prop_conv)) +
#   geom_tile(color = 'grey50') +
#   xlab('Detection') +
#   ylab('Sample size') +
#   facet_nested(type ~ survey_freq, drop = T,
#              # labeller = labeller(survey_freq = freq_labels),
#              scales = 'free_x') +
#   scale_fill_gradient2(name = "Convergence",
#                       mid = "#FFFFFF", high = "deepskyblue1", low = "red2", midpoint = 0.95) +
#   theme_bw() +
#   theme(legend.position = 'right',
#              strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
#         legend.text = element_text(size = 8, vjust = 0.5),
#         legend.title = element_text(vjust = 0.8, size = 10)) 

phi_pars <- c('phi[P]','phi[1]', 'phi[2]', 'phi[J]', 'phi[A]')
p_pars <- c('p[1]', 'p[2]', 'p[J]', 'p[A]') 
freq_labels <- c('1' = 'Annual', '2' = 'Biennial', '3' = 'Triennial')
occ_labels <- c('10' = '10 yrs', '20' = '20 yrs')


```

### Results  
Results for each of the model performance metrics are described below. In general, it is important to note that the estimation of pup and adult survival often show the strongest precision/bias patterns across the scenarios, but likely for two different reasons. Pup survival is intentionally simulated to have higher variability to reflect reality, and therefore may have poorer precision or increased bias. Adult 5+ survival, on the other hand, has minimal simulated variability but will always be informed by fewer branded cohorts, which can be influential in the shorter study time horizon (e.g., adult survival would only be informed by 2 cohorts in a 10-year study with triennial branding). 

#### Big-picture take-aways  
- Results are consistent across the different performance metrics, though the strength of the apparent patterns varies across metrics.  
- Model performance degrades most notably across branding frequency, cohort sampling size, and study horizon, with smaller differences across detection levels.  
- In terms of getting accurate and unbiased estiamtes of a mean survival probability, precision goals stated in MMPA permits can be reliably achieved with either a 20-year study horizon or annual branding. 
- The probability of detecting a change in survival varies considerably across simulation scenarios and highlights the power of increasing branding frequency and cohort sample sizes if detecting a change in survival is a high research priority (as opposed to just having precise mean survival estimates). 


#### Coefficient of variation   
The coefficient of variation (CV) is a relative measure of precision (i.e., how dispersed estimates are around the mean) that is used in MMPA permit applications for the sea lion branding program. The CV values shown below follow an expected pattern across survey features, with the most precise estimates in larger sample sizes, the longer study horizon, and more frequent branding. 

In examining the probability of the CV being below 0.125 (threshold used in MMPA permit applications for sea lion branding) across all simulations, we can see the importance of a longer study time horizon. The MMPA permit application benchmark can be achieved reliably for juveniles and adults in all scenarios and achieved reliably for younger age classes in scenarios with: (1) annual branding, (2) biennial branding over a longer study, or (3) triennial branding over a longer study with larger sample sizes. In short, sample size can compensate for other "losses" of information to a degree that increasing detection probability (within the range studied here) cannot. 



```{r results cv}

cv.files7 <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'cv7', all.files = FALSE, full.names = F)

cv.files8 <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'cv8', all.files = FALSE, full.names = F)

cv.files9 <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'cv9', all.files = FALSE, full.names = F)

cv.files10 <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'cv10', all.files = FALSE, full.names = F)

cv.files11 <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'cv11', all.files = FALSE, full.names = F)

cv.files12 <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'cv12', all.files = FALSE, full.names = F)

cv.files <- c(cv.files7, cv.files8, cv.files9, cv.files10, cv.files11, cv.files12)


cv <- data.frame()
for (f in 1:length(cv.files)) {
temp.cv <- data.frame(readRDS(file = here::here('results', 'simpleAge', 'sims',
                                                  cv.files[f])))
#use gsub on cv files rather than use different list to make sure they match
rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge', 'sims', 
                                                     gsub('cv', 'median', cv.files[f])))[,'max_rhat'])

temp.cv$rhat <- as.numeric(rhats$rhat)
temp <- temp.cv %>% 
  reshape2::melt(id.vars = 'rhat') %>%
  transform(scenario = parse_number(gsub('S.', '', cv.files[f]))) %>%
  transform(value = ifelse(is.nan(value), 0, value))

cv <- rbind(cv, temp)

} #rds file

cv <- cv %>%
  filter(rhat < 1.1) 

mean.cv <- data.frame(cv) %>%
  group_by(variable, scenario) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(variable, detection, N, survey_freq) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA',
                            'p1', 'p2', 'pJ', 'pA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') %>%
  transform(length = 10)

mean.cv.thresh <- data.frame(cv) %>%
  transform(value_cat = value<0.125) %>%
  group_by(variable, scenario) %>%
  dplyr::summarize(value_cat = mean(value_cat), .groups = 'keep') %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(variable, detection, N, survey_freq) %>%
  dplyr::summarize(value_cat = mean(value_cat), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') %>%
  transform(length = 10)


```


```{r results cv prepop, fig.height = 6, fig.width = 7}

cv.files10 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv10', all.files = FALSE, full.names = F)

cv.files11 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv11', all.files = FALSE, full.names = F)

cv.files14 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv14', all.files = FALSE, full.names = F)

cv.files15 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv15', all.files = FALSE, full.names = F)

cv.files18 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv18', all.files = FALSE, full.names = F)

cv.files19 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv19', all.files = FALSE, full.names = F)

cv.files22 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'),
                         pattern = 'cv22', all.files = FALSE, full.names = F)

cv.files23 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv23', all.files = FALSE, full.names = F)

cv.files26 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv26', all.files = FALSE, full.names = F)

cv.files27 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv27', all.files = FALSE, full.names = F)

cv.files30 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv30', all.files = FALSE, full.names = F)

cv.files31 <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'cv31', all.files = FALSE, full.names = F)


cv.files <- c(cv.files10, cv.files11, cv.files14, cv.files15, cv.files18, cv.files19,
              cv.files22, cv.files23, cv.files26, cv.files27, cv.files30, cv.files31)

cv.20 <- data.frame()
for (f in 1:length(cv.files)) {
temp.cv <- data.frame(readRDS(file = here::here('results', 'simpleAge', 'sims', 'prePop',
                                                  cv.files[f])))
#use gsub on cv files rather than use different list to make sure they match
rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge', 'sims', 'prePop',
                                                     gsub('cv', 'median', cv.files[f])))[,'max_rhat'])

temp.cv$rhat <- as.numeric(rhats$rhat)
temp <- temp.cv %>% 
  reshape2::melt(id.vars = 'rhat') %>%
  transform(scenario = parse_number(gsub('S.', '', cv.files[f]))) %>%
  transform(value = ifelse(is.nan(value), 0, value))

cv.20 <- rbind(cv.20, temp)

} #rds file

cv.20 <- cv.20 %>%
  filter(rhat < 1.1) #%>%
  # transform(length = 20) #%>%
  # bind_rows(cv)

mean.cv.20 <- data.frame(cv.20) %>%
  group_by(variable, scenario) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(variable, detection, N, survey_freq) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA',
                            'p1', 'p2', 'pJ', 'pA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') %>%
  transform(length = 20) %>%
  bind_rows(mean.cv)


#phi
plot_dat <- mean.cv.20 %>% filter(grepl('phi', variable)) %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'), 
                              labels = phi_pars))

ggplot(plot_dat, aes(x = factor(detection), y = variable, fill = value)) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N ~ survey_frequency + survey_freq + length, drop = T, 
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
 scale_fill_gradient2(name = "cv",
                      mid = "#FFFFFF", low = "#012345", high = "#012345", midpoint = 0) +
  theme_bw() +
  theme(legend.position = 'right',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
        legend.text = element_text(size = 8, vjust = 0.5),
        legend.title = element_text(vjust = 0.8, size = 10)) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable))))))

#threshold approach
mean.cv.thresh.20 <- data.frame(cv %>% transform(length = 10)) %>% 
  bind_rows(cv.20 %>% transform(length = 20)) %>%
  transform(value_cat = value<0.125) %>%
  group_by(variable, scenario, length) %>%
  dplyr::summarize(value_cat = mean(value_cat), .groups = 'keep') %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(variable, detection, N, survey_freq, length) %>%
  dplyr::summarize(value_cat = mean(value_cat), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size')

plot_dat <- mean.cv.thresh.20 %>% filter(grepl('phi', variable)) %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'), 
                              labels = phi_pars))

ggplot(plot_dat, aes(x = factor(detection), y = variable, fill = value_cat)) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N ~ survey_frequency + survey_freq + length, drop = T, 
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
scale_fill_gradient2(name = "Probability of \n cv < 0.125",
                  mid = "#FFFFFF", high = "deepskyblue1", low = "red2", midpoint = 0.8) +
  theme_bw() +
  theme(legend.position = 'right',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
        legend.text = element_text(size = 8, vjust = 0.5),
        legend.title = element_text(vjust = 0.8, size = 10)) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable))))))

#dotplots
#continuous raw
dotplot_dat <- cv.20 %>% transform(length = 20) %>% 
  bind_rows(cv %>% transform(length = 10)) %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(length, variable, detection, survey_freq, N) %>%
  # dplyr::summarize(value = mean(value),
  #                  lower = quantile(value, probs = 0.025),
  #                  upper = quantile(value, probs = 0.975),
    dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency', samp_size = 'Sample size') %>%
  #filter out detection params
  filter(!is.na(variable))


# ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & variable == 'phiP'),
#        aes(x = detection, y = value, col = factor(length), group = length)) +
#   geom_point() + geom_line() +
#   geom_hline(aes(yintercept = 0), linetype = 'dotted') +
#   xlab('Detection') + ylab('CV') +
#     # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
#   # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +
# 
#   facet_nested(variable + samp_size + N ~ survey_frequency + survey_freq, drop = T,
#              labeller = labeller(survey_freq = freq_labels, length = occ_labels),
#              scales = 'free_x') +
# 
#   theme_bw() +
#   plot_theme(legend.position = 'top',
#              plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
#   scale_color_manual(values = rainbow2[c(5,2)], name = 'Years')

#no study horizon
ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & length == 20),
       aes(x = factor(survey_freq), y = value, col = factor(variable), group = variable)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding') + ylab('CV') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +
  facet_nested(detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2, name = 'Rate')

#study horizon
ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & variable %in% c('phiP', 'phi1')),
       aes(x = factor(survey_freq), y = value, col = factor(length), group = length)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding') + ylab('CV') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +
  facet_nested(variable + detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2, name = 'Rate')

#categorical threshold 
dotplot_dat <- cv.20 %>% transform(length = 20) %>% 
  bind_rows(cv %>% transform(length = 10)) %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  transform(value_cat = value<0.125) %>%
  group_by(length, variable, detection, survey_freq, N) %>%
  dplyr::summarize(value_cat = mean(value_cat), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency', samp_size = 'Sample size') %>%
  #filter out detection params
  filter(!is.na(variable))


ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & variable == 'phiP'),
       aes(x = detection, y = value_cat, col = factor(length), group = length)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Detection') + ylab('Probability of meeting CV threshold') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +

  facet_nested(variable + samp_size + N ~ survey_frequency + survey_freq, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +

  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2[c(5,2)], name = 'Years')

#no study horizon
ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & length == 20),
       aes(x = factor(survey_freq), y = value_cat, col = factor(variable), group = variable)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding') + ylab('CV') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +
  facet_nested(detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2, name = 'Rate')

#study horizon
ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & variable %in% c('phiP', 'phi1')),
       aes(x = factor(survey_freq), y = value_cat, col = factor(length), group = length)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding') + ylab('CV') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +
  facet_nested(variable + detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2, name = 'Rate')


```

##### "Gains in information"  
A more collapsed version of the results that tries to convey gains in performance when "more information" is given to the model relative to the "worse case" survey conditions (triennial branding with low detection). It shows that investing in increasing branding frequency does more for estimation precision compared with increasing detection probabilities.  

It is important to note that this depiction of "gains in information" does not span all survey design options and shows gains in performance when you take sequential and joint steps of improvement. For example, this figure shows model performance with medium detection and biennial branding, or high detection and annual branding and does NOT show performance, for example, at medium detection and annual branding. 

```{r results cv delta}

#mean.cv.thresh.20 contains the mean probability (across simulations) where the cv < 0.125.

#values for the scenario with the "least" amount of contributing information
#i.e., triennial sampling, N = 50, 10 years, low detection (scenario 31)

#just look at 20 years for now
cv_least_info_50 <- mean.cv.thresh.20 %>%
  filter(detection == 'Low' & length == 20 & survey_freq == 3 & N == 50) %>%
  dplyr::select(c(variable, value_cat)) %>%
  dplyr::rename(min_val = value_cat)

cv_delta_50 <- mean.cv.thresh.20 %>% filter(length == 20) %>%
  merge(cv_least_info_50, by = c('variable')) %>%
  transform(delta = value_cat-min_val) %>%
  filter(N %in% c(50, 100, 200)) %>% 
  transform(det_cat = as.numeric(factor(detection)),
            N_cat = as.numeric(factor(N))) %>%
  filter(variable == 'phiP') %>%
  # filter(variable %in% c('phiP', 'phiA')) %>%
  transform(survey_cat = 4-as.numeric(factor(survey_freq))) %>%
  transform(step = N_cat + survey_cat + det_cat) %>%
  transform(step = ifelse(detection == 'Low' & length == 20 & survey_freq == 3 & N == 50, 0, step))

#this would be more meaningful on the straight probabilities rather than the deltas
# ggplot(cv_delta_50, aes(detection, delta, 
#                      color = factor(survey_freq), 
#                      group = variable,
#                      shape = variable)) +
#   geom_point() +
#   geom_line() +
#   facet_grid(survey_freq~N) 

#different idea
cv_least_info <- mean.cv.thresh.20 %>%
  filter(detection == 'Low' & length == 20 & survey_freq == 3 & N %in% c(50, 100, 200)) %>%
  dplyr::select(c(variable, N, value_cat)) %>%
  dplyr::rename(min_val = value_cat)

cv_delta <- mean.cv.thresh.20 %>% filter(length == 20) %>%
    filter(N %in% c(50, 100, 200)) %>% 
  merge(cv_least_info, by = c('variable', 'N')) %>%
  transform(delta = value_cat-min_val) %>%
  transform(det_cat = as.numeric(factor(detection)),
            N_cat = as.numeric(factor(N))) %>%
  # filter(variable == 'phiP') %>%
  # filter(variable %in% c('phiP', 'phi1')) %>%
  transform(survey_cat = 4-as.numeric(factor(survey_freq))) %>%
  transform(step = N_cat + survey_cat + det_cat) %>%
  transform(step = ifelse(detection == 'Low' & length == 20 & survey_freq == 3, 0, step))

cv_delta_steps <- cv_delta %>% 
  # filter(N == 50 & variable == 'phiP') %>%
    # filter(variable %in% c('phiP', 'phi1')) %>%
  #because of filters, N is always 1 - update if more complex
  transform(category = ifelse(step == 0, 'Baseline', 
                  ifelse(det_cat > 1 & survey_cat == 1, 'Detection only',
                  ifelse(survey_cat > 1 & det_cat == 1, 'Frequency only', 'Detection & frequency')))) %>%
  transform(step = ifelse(step == 0, 0,
                          ifelse(det_cat == 1 & survey_cat == 1, NA,
                            ifelse(category == 'Detection only' & N_cat == 1 & det_cat == 2 | 
                            category == 'Frequency only' & N_cat == 1 & survey_cat == 2 |
                              survey_cat == 2 & N_cat == 1 & det_cat == 2, 1,
                          ifelse(category == 'Detection only' & N_cat == 1 & det_cat == 3 | 
                            category == 'Frequency only' & N_cat == 1 & survey_cat == 3 |
                              survey_cat == det_cat & N_cat == 1, 2, 
                            ifelse(category == 'Detection only' & N_cat == 2 & det_cat == 2 | 
                            category == 'Frequency only' & N_cat == 2 & survey_cat == 2 |
                              survey_cat == 2 & N_cat == 2 & det_cat == 2, 1,
                          ifelse(category == 'Detection only' & N_cat == 2 & det_cat == 3 | 
                            category == 'Frequency only' & N_cat == 2 & survey_cat == 3 |
                              survey_cat == det_cat & N_cat == 2, 2, 
                            ifelse(category == 'Detection only' & N_cat == 3 & det_cat == 2 | 
                            category == 'Frequency only' & N_cat == 3 & survey_cat == 2 |
                              survey_cat == 2 & N_cat == 3 & det_cat == 2, 1,
                          ifelse(category == 'Detection only' & N_cat == 3 & det_cat == 3 | 
                            category == 'Frequency only' & N_cat == 3 & survey_cat == 3 |
                              survey_cat == det_cat & N_cat == 3, 2, 
                            ifelse(det_cat == 1 & survey_cat == 1, NA, NA)))))))))) %>%
  transform(category = factor(category, levels = c('Baseline', 'Detection only', 
                                                   'Frequency only', 
                                                   'Detection & frequency')))

#shows change in probability relative to the "worst case" survey conditions
# ggplot(cv_delta_steps %>% filter(!is.na(step)), 
#        aes(factor(step), delta, group = category, col = category)) +
#   # geom_point(position = position_dodge(width = 0.1)) +
#   geom_point() +
#   geom_line() +
#   geom_hline(aes(yintercept = 0), linetype = 'dotted') +
#   facet_grid(variable~N, scales = 'free') +
#   xlab('Stepwise gain in information') + ylab('Change in probability') +
#   theme_bw() +
#   theme(legend.position = 'top',
#              strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
#         legend.text = element_text(size = 8, vjust = 0.5),
#         legend.title = element_text(vjust = 0.8, size = 10)) +
#     scale_color_manual(values = rainbow2[c(1,2,5,6)], name = '')

#simplify to "fill" the baseline scenario
scenario_fill <- data.frame(step = c(rep(0, 9)),
                            # value_cat = rep(1,9),
                            N = c(rep(c(50,100,200), each = 3)),
                            category = c(rep(c('Detection only', 'Frequency only', 'Detection & frequency'), times = 3))) %>%
  merge(cv_delta_steps %>% dplyr::select(min_val, category, variable, N), 
        by = c('category', 'N'), all.x = T) %>% 
  distinct() %>%
  dplyr::rename(value_cat = min_val)
                            
plot_dat <- cv_delta_steps %>%
  dplyr::select(category, N, step, value_cat, variable) %>%
  bind_rows(scenario_fill) %>%
  transform(samp_size = 'Sample size') %>%
  filter(category != 'Baseline') %>%
  transform(category = factor(category, levels = c('Baseline', 'Detection only', 
                                                   'Frequency only', 
                                                   'Detection & frequency'))) 

#just shows straight probability of being under the cv threshold.... 
#this is better because it shows the different relative starting points of the sample sizes
ggplot(plot_dat %>% filter(!is.na(step)), 
       aes(factor(step), value_cat, group = category, col = category)) +
  # geom_point(position = position_dodge(width = 0.1)) +
  geom_point() +
  geom_line() +
  facet_nested(variable~samp_size+N) +
  xlab('Stepwise gain in information') + ylab('Probability of meeting CV threshold') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
        legend.text = element_text(size = 8, vjust = 0.5),
        legend.title = element_text(vjust = 0.8, size = 10)) +
    scale_color_manual(values = rainbow2[c(1,2,5,6)], name = '')

```


#### Detecting changes in survival  
As noted above, I examined the ability to detect a "small" decrease (15%, 10%, 5%, 2.5%) and a "big" decrease (20%, 15%, 10%, 5%) in survival for pups, age-1:2, juveniles, and adults, respectively. 

```{r results threshold}

cri.scenarios <- c('dec_little', 'dec_big')
append <- c(5,10)

cri.all <- data.frame()

for (a in 1:length(append)) { #files for period length

for (j in 1:length(cri.scenarios)) {
  
cri.files <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = paste0(cri.scenarios[j],append[a]), all.files = FALSE, full.names = F)

cri <- data.frame()
for (f in 1:length(cri.files)) {
temp.cri <- data.frame(readRDS(file = here::here('results', 'simpleAge', 'sims',
                                                  cri.files[f])))
#use gsub on cri files rather than use different list to make sure they match
rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge', 'sims',
                                                     gsub('median_', 'median', gsub(paste0(cri.scenarios[j], append[a]), 'median', cri.files[f]))))[,'max_rhat'])

temp.cri$rhat <- as.numeric(rhats$rhat)
temp <- temp.cri %>% 
  reshape2::melt(id.vars = 'rhat') %>%
  transform(scenario = parse_number(gsub('S.', '', cri.files[f]))) %>%
  transform(run = str_sub(cri.files[f], -5,-5)) %>%
  transform(length = append[a]) %>%
  # transform(length = ifelse(grepl('little5', cri.files[f]) | grepl('big5', cri.files[f]), 5, 10)) %>%
  transform(threshold = cri.scenarios[j])

cri <- rbind(cri, temp)

} #appending length of year period

cri.all <- rbind(cri, cri.all)

} #rds file
} #j


mean.cri <- data.frame(cri.all) %>%
  filter(!is.na(value)) %>%
    merge(scenarios, by = 'scenario', all = F) %>%
  group_by(threshold, variable, detection, N, survey_freq, length) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA',
                            'p1', 'p2', 'pJ', 'pA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') 

plot_dat <- mean.cri %>% filter(grepl('phi', variable) & length %in% c(10,20)) %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'), 
                              labels = phi_pars)) %>%
  transform(value_cat = ifelse(value < 0.5, "<0.5",
                               ifelse(value >=0.5 & value < 0.8, '0.5-0.8', '>0.8'))) %>%
  transform(value_cat = factor(value_cat, levels = c('<0.5', '0.5-0.8', '>0.8'))) %>%
  transform(threshold = factor(threshold, levels = c('dec_little', 'dec_big', 'inc_little', 'inc_big'),
                               labels = c('Small decrease', 'Big decrease', 'Small increase', 'Big increase'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') 


#continuous color scale
#big decrease in survival 
# ggplot(plot_dat %>% filter(threshold %in% c('Big decrease')), 
#        aes(x = factor(detection), y = variable, fill = value)) +
#         # aes(x = factor(detection), y = variable, fill = value_cat)) +
#   geom_tile(color = 'grey50') +
#   xlab('Detection') +
#   ylab('Parameter') +
#   facet_nested(samp_size + N ~ survey_frequency + survey_freq + length, drop = T, 
#              labeller = labeller(survey_freq = freq_labels, length = occ_labels),
#              scales = 'free_x') +
# scale_fill_gradient2(name = "Probability of detecting change",
#                   mid = "#FFFFFF", high = "deepskyblue1", low = "red2", midpoint = 0.8) +
#   theme_bw() +
#   theme(legend.position = 'top',
#              strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF")) +
#     scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable)))))) 


```

```{r results threshold prepop, fig.width = 10, fig.height = 7}

library(stringr)

cri.scenarios <- c('dec_little', 'dec_big', 'inc_little', 'inc_big')
cri.scenarios <- c('dec_little', 'dec_big')
append <- c(5,10)

cri.all.20 <- data.frame()

for (a in 1:length(append)) { #files for period length

for (j in 1:length(cri.scenarios)) {
  
cri.files <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = paste0(cri.scenarios[j],append[a]), all.files = FALSE, full.names = F)

cri <- data.frame()
for (f in 1:length(cri.files)) {
temp.cri <- data.frame(readRDS(file = here::here('results', 'simpleAge', 'sims', 'prePop',
                                                  cri.files[f])))
#use gsub on cri files rather than use different list to make sure they match
rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge', 'sims', 'prePop', 
                                                     gsub('median_', 'median', gsub(paste0(cri.scenarios[j], append[a]), 'median', cri.files[f]))))[,'max_rhat'])

temp.cri$rhat <- as.numeric(rhats$rhat)
temp <- temp.cri %>% 
  reshape2::melt(id.vars = 'rhat') %>%
  transform(scenario = parse_number(gsub('S.', '', cri.files[f]))) %>%
  transform(run = str_sub(cri.files[f], -6,-5)) %>%
  transform(length = append[a]+10) %>%
  # transform(length = ifelse(grepl('little5', cri.files[f]) | grepl('big5', cri.files[f]), 5, 10)) %>%
  transform(threshold = cri.scenarios[j])

cri <- rbind(cri, temp)

} #appending length of year period

cri.all.20 <- rbind(cri, cri.all.20)

} #rds file
} #j

mean.cri.20 <- data.frame(cri.all.20) %>%
  #are these just all the unfinished/unfilled sim dataframe rows? 
  filter(!is.na(value)) %>%
    merge(scenarios, by = 'scenario', all = F) %>%
  group_by(threshold, variable, detection, N, survey_freq, length) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  bind_rows(mean.cri)


plot_dat <- mean.cri.20 %>% filter(grepl('phi', variable) & length %in% c(10,20)) %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'), 
                              labels = phi_pars)) %>%
  transform(value_cat = ifelse(value < 0.5, "<0.5",
                               ifelse(value >=0.5 & value < 0.8, '0.5-0.8', '>0.8'))) %>%
  transform(value_cat = factor(value_cat, levels = c('<0.5', '0.5-0.8', '>0.8'))) %>%
  transform(threshold = factor(threshold, levels = c('dec_little', 'dec_big', 'inc_little', 'inc_big'),
                               labels = c('Small decrease', 'Big decrease', 'Small increase', 'Big increase'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') 


#continuous color scale
#big decrease in survival 
cri_big <- ggplot(plot_dat %>% filter(threshold %in% c('Big decrease')), 
       aes(x = factor(detection), y = variable, fill = value)) +
        # aes(x = factor(detection), y = variable, fill = value_cat)) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N ~ survey_frequency + survey_freq + length, drop = T, 
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
scale_fill_gradient2(name = "Probability of detecting change",
                  mid = "#FFFFFF", high = "deepskyblue1", low = "red2", midpoint = 0.8) +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF")) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable)))))) 

#categorical
cri_small_cat <- ggplot(plot_dat %>% filter(threshold %in% c('Small decrease')),
       aes(x = factor(detection), y = variable, fill = value_cat), alpha = 0.3) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N ~ survey_frequency + survey_freq + length, drop = T, 
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF")) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable)))))) +
  scale_fill_manual(name = "Probability of detecting small change",
                    # values = c('firebrick3', 'orange', 'steelblue2'))
                    values = c('salmon', 'lightgoldenrod2', 'lightskyblue2'))

cri_big_cat <- ggplot(plot_dat %>% filter(threshold %in% c('Big decrease')),
       aes(x = factor(detection), y = variable, fill = value_cat), alpha = 0.3) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N ~ survey_frequency + survey_freq + length, drop = T, 
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF")) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable)))))) +
  scale_fill_manual(name = "Probability of detecting large change",
                    # values = c('firebrick3', 'orange', 'steelblue2'))
                    values = c('salmon', 'lightgoldenrod2', 'lightskyblue2'))

#detection only - least influential facet
#choose detection level
cri_highD_cat <- ggplot(plot_dat %>% filter(detection == 'Med'),
       aes(x = factor(N), y = variable, fill = value_cat), alpha = 0.3) +
  geom_tile(color = 'grey50') +
  xlab('Sample size') +
  ylab('Parameter') +
  facet_nested(threshold ~ survey_frequency + survey_freq + length, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF")) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable)))))) +
  scale_fill_manual(name = "Probability of detecting change in survival",
                    values = c('salmon', 'lightgoldenrod2', 'lightskyblue2'))

#dot plots
dotplot_dat <- cri.all.20 %>% transform(length = 20) %>% 
  bind_rows(cri.all %>% transform(length = 10)) %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(length, variable, detection, survey_freq, N, threshold) %>%
  # dplyr::summarize(value = mean(value),
  #                  lower = quantile(value, probs = 0.025),
  #                  upper = quantile(value, probs = 0.975),
    dplyr::summarize(value = mean(value, na.rm = T), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency', samp_size = 'Sample size') %>%
  #filter out detection params
  filter(!is.na(variable))


ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & variable == 'phiP'),
       aes(x = survey_freq, y = value, col = factor(length), group = length)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding frequency') + ylab('Probability of detecting change') +
  facet_nested(threshold + detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2[c(1,5,2)], name = 'Years')

#no study horizon
ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & length == 20),
       aes(x = factor(survey_freq), y = value, col = factor(variable), group = variable)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding') + ylab('CV') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +
  facet_nested(detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2, name = 'Rate')

#study horizon
ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & variable %in% c('phiP', 'phi1')),
       aes(x = factor(survey_freq), y = value, col = factor(length), group = length)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding') + ylab('CV') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +
  facet_nested(variable + detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2, name = 'Rate')


```

```{r more facets, fig.width = 9, fig.height = 12}

plot_dat <- mean.cri.20 %>% filter(grepl('phi', variable)) %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'), 
                              labels = phi_pars)) %>%
  transform(value_cat = ifelse(value < 0.5, "<0.5",
                               ifelse(value >=0.5 & value < 0.8, '0.5-0.8', '>0.8'))) %>%
  transform(value_cat = factor(value_cat, levels = c('<0.5', '0.5-0.8', '>0.8'))) %>%
  transform(threshold = factor(threshold, levels = c('dec_little', 'dec_big', 'inc_little', 'inc_big'),
                               labels = c('Small decrease', 'Big decrease', 'Small increase', 'Big increase'))) %>%
  transform(survey_frequency = 'Branding frequency & change size', samp_size = 'Sample size & study length')

#more facets
cri_all_cont <- ggplot(plot_dat %>% filter(length %in% c(10,20)),
       aes(x = factor(detection), y = variable, fill = value)) +
        # aes(x = factor(detection), y = variable, fill = value_cat)) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N + length ~ survey_frequency + survey_freq + threshold, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
scale_fill_gradient2(name = "Probability of detecting change",
                  mid = "#FFFFFF", high = "deepskyblue1", low = "red2", midpoint = 0.8) +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF")) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable))))))


cri_all_cat <- ggplot(plot_dat %>% filter(length %in% c(10,20)),
       aes(x = factor(detection), y = variable, fill = value_cat)) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N + length ~ survey_frequency + survey_freq + threshold, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF")) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable)))))) +
  scale_fill_manual(name = "Probability of detecting change in survival",
                    # values = c('firebrick3', 'orange', 'steelblue2'))
                    values = c('salmon', 'lightgoldenrod2', 'lightskyblue2'))

cri_20_cat <- ggplot(plot_dat %>% filter(length %in% c(20)),
       aes(x = factor(detection), y = variable, fill = value_cat)) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N + length ~ survey_frequency + survey_freq + threshold, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF")) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable)))))) +
  scale_fill_manual(name = "Probability of detecting change in survival",
                    # values = c('firebrick3', 'orange', 'steelblue2'))
                    values = c('salmon', 'lightgoldenrod2', 'lightskyblue2'))

```

##### Detecting changes across all variables  
This is a lot to look at, but overall, you can see that in order to achieve >80% probability of detecting a change in survival, the most influential survey features are branding schedule, sample size, study horizon, and whether you're interested in detecting a smaller or larger change in survival. 

```{r cri all facets, fig.width = 9, fig.height = 10}

cri_all_cat

```

##### Simpler view: detecting smaller or larger changes 
In the following figures, the above information is filtered for simpler views. Here we can see a clearer picture of the effects of study horizon, sample size, and branding frequency. These results show that a larger change in survival can be reliably detected in a variety of scenarios, including triennial sampling of larger cohorts over a 20-year study. However, if you want to detect a smaller change in survival, annual branding or biennial branding with larger sample sizes and a 20-year study are necessary. 

```{r cri fewer facets I, fig.width = 7, fig.height = 6}

cri_big_cat
cri_small_cat


```

##### Simpler view: detecting changes at medium detection  
This figure provides a reoriented perspective by examining probabilities of detecting small and large changes across all survey features and medium detection levels.  

```{r cri fewer facets II, fig.width = 9, fig.height = 4}

cri_highD_cat

```

##### Simpler view: detecting changes at 20 years  
This figure is another simpler view that shows all of the facets but just a 20-year study time horizon.  

```{r cri fewer facets III, , fig.width = 7, fig.height = 6}

cri_20_cat

```


##### "Gains in information"  
Similar to the above, these figures show the probability of detecting a change in survival collapsed across the survey effort features.  

```{r results threshold delta}

#mean.cv.thresh.20 contains the mean probability (across simulations) where the cv < 0.125.

#values for the scenario with the "least" amount of contributing information
#i.e., triennial sampling, N = 50, 10 years, low detection (scenario 31)

cri_least_info <- mean.cri.20 %>% transform(samp_size = 'Sample size') %>%
  dplyr::select(!survey_frequency) %>%
  filter(detection == 'Low' & length %in% c(10,20) &
           survey_freq == 3 & N %in% c(50, 100, 200)) %>%
  dplyr::select(c(variable, N, value, length, threshold)) %>%
  dplyr::rename(min_val = value)

cri_delta <- mean.cri.20 %>% filter(length %in% c(10,20)) %>%
    filter(N %in% c(50, 100, 200)) %>% 
  merge(cri_least_info, by = c('variable', 'N', 'length', 'threshold')) %>%
  transform(delta = value-min_val) %>%
  transform(det_cat = as.numeric(factor(detection)),
            N_cat = as.numeric(factor(N))) %>%
  # filter(variable == 'phiP') %>%
  filter(variable %in% c('phiP', 'phi1')) %>%
  transform(survey_cat = 4-as.numeric(factor(survey_freq))) %>%
  transform(step = N_cat + survey_cat + det_cat) %>%
  transform(step = ifelse(detection == 'Low' & survey_freq == 3, 0, step))

cri_delta_steps <- cri_delta %>% 
  # filter(N == 50 & variable == 'phiP') %>%
    filter(variable %in% c('phiP', 'phi1')) %>%
  transform(category = ifelse(step == 0, 'Baseline', 
                  ifelse(det_cat > 1 & survey_cat == 1, 'Detection only',
                  ifelse(survey_cat > 1 & det_cat == 1, 'Frequency only', 'Detection & frequency')))) %>%
  transform(step = ifelse(step == 0, 0,
                          ifelse(det_cat == 1 & survey_cat == 1, NA,
                            ifelse(category == 'Detection only' & N_cat == 1 & det_cat == 2 | 
                            category == 'Frequency only' & N_cat == 1 & survey_cat == 2 |
                              survey_cat == 2 & N_cat == 1 & det_cat == 2, 1,
                          ifelse(category == 'Detection only' & N_cat == 1 & det_cat == 3 | 
                            category == 'Frequency only' & N_cat == 1 & survey_cat == 3 |
                              survey_cat == det_cat & N_cat == 1, 2, 
                            ifelse(category == 'Detection only' & N_cat == 2 & det_cat == 2 | 
                            category == 'Frequency only' & N_cat == 2 & survey_cat == 2 |
                              survey_cat == 2 & N_cat == 2 & det_cat == 2, 1,
                          ifelse(category == 'Detection only' & N_cat == 2 & det_cat == 3 | 
                            category == 'Frequency only' & N_cat == 2 & survey_cat == 3 |
                              survey_cat == det_cat & N_cat == 2, 2, 
                            ifelse(category == 'Detection only' & N_cat == 3 & det_cat == 2 | 
                            category == 'Frequency only' & N_cat == 3 & survey_cat == 2 |
                              survey_cat == 2 & N_cat == 3 & det_cat == 2, 1,
                          ifelse(category == 'Detection only' & N_cat == 3 & det_cat == 3 | 
                            category == 'Frequency only' & N_cat == 3 & survey_cat == 3 |
                              survey_cat == det_cat & N_cat == 3, 2, 
                            ifelse(det_cat == 1 & survey_cat == 1, NA, NA)))))))))) 

scenario_fill_cri <- expand.grid(step = 0,
              length = c(10,20),
              threshold = c('dec_little', 'dec_big'),
              N = c(50, 100, 200),
              category = c('Detection only', 'Frequency only', 
                                         'Detection & frequency')) %>%
  merge(cri_delta_steps %>% dplyr::select(length, threshold, min_val, category, variable, N), 
        by = c('category', 'N', 'length', 'threshold')) %>% 
  distinct() %>%
  dplyr::rename(value = min_val)
                            
plot_dat <- cri_delta_steps %>%
  dplyr::select(category, N, length, threshold, step, value, variable) %>%
  bind_rows(scenario_fill_cri) %>%
  filter(category != 'Baseline') %>%
  transform(threshold_name = factor(threshold, levels = c('dec_little', 'dec_big'), 
                                    labels = c('Small decrease', 'Large decrease'))) %>%
  transform(category = factor(category, levels = c('Baseline', 'Detection only', 
                                                   'Frequency only', 
                                                   'Detection & frequency'))) 

#just shows straight probability of detecting change.... 
ggplot(plot_dat %>% filter(!is.na(step)) %>% filter(threshold == 'dec_little'), 
       aes(factor(step), value, group = category, col = category)) +
  # geom_point(position = position_dodge(width = 0.1)) +
  geom_point() +
  geom_line() +
  facet_nested(variable+length~N, scales = 'free') +
  xlab('Stepwise gain in information') + ylab('Probability of detecting small change') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
        legend.text = element_text(size = 8, vjust = 0.5),
        legend.title = element_text(vjust = 0.8, size = 10)) +
    scale_color_manual(values = rainbow2[c(1,2,5,6)], name = '')

ggplot(plot_dat %>% filter(!is.na(step)) %>% filter(threshold == 'dec_big'), 
       aes(factor(step), value, group = category, col = category)) +
  # geom_point(position = position_dodge(width = 0.1)) +
  geom_point() +
  geom_line() +
  facet_nested(variable+length~N, scales = 'free') +
  xlab('Stepwise gain in information') + ylab('Probability of detecting large change') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
        legend.text = element_text(size = 8, vjust = 0.5),
        legend.title = element_text(vjust = 0.8, size = 10)) +
    scale_color_manual(values = rainbow2[c(1,2,5,6)], name = '')

#
ggplot(plot_dat %>% filter(!is.na(step)) %>% filter(variable == 'phiP') %>%
         transform(samp_size = 'Sample size & study length') %>%
         transform(length = factor(length, labels = c('10 yrs', '20 yrs'))), 
       aes(factor(step), value, group = category, col = category)) +
  # geom_point(position = position_dodge(width = 0.1)) +
  geom_point() +
  geom_line() +
  facet_nested(threshold_name~samp_size+N+length) +
  xlab('Stepwise gain in information') + ylab('Probability of detecting change in pup survival') +
  theme_bw() +
  theme(legend.position = 'top',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
        legend.text = element_text(size = 8, vjust = 0.5),
        legend.title = element_text(vjust = 0.8, size = 10)) +
    scale_color_manual(values = rainbow2[c(1,2,5,6)], name = '')

```


#### Relative bias  
Relative bias was generally low for all scenarios, though higher relative bias is evident with triennial branding and small sample size. Patterns in bias for survival estimates are less apparent across levels of detection.


```{r results bias}

bias.files <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'bias', all.files = FALSE, full.names = F)

rel.bias <- data.frame()
for (f in 1:length(bias.files)) {
temp.bias <- data.frame(readRDS(file = here::here('results', 'simpleAge', 'sims',
                                                  bias.files[f])))
#use gsub on bias files rather than use different list to make sure they match
rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge', 'sims',
                                                     gsub('bias', 'median', bias.files[f])))[,'max_rhat'])

temp.bias$rhat <- as.numeric(rhats$rhat)
temp <- temp.bias %>% 
  reshape2::melt(id.vars = 'rhat') %>%
  transform(scenario = parse_number(gsub('S.', '', bias.files[f]))) %>%
  transform(value = ifelse(is.nan(value), 0, value))

rel.bias <- rbind(rel.bias, temp)

} #rds file

rel.bias <- rel.bias %>%
  filter(rhat < 1.1) %>%
  transform(length = 10)

mean.rel.bias <- data.frame(rel.bias) %>%
  group_by(variable, scenario) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(variable, detection, N, survey_freq) %>%
  dplyr::summarize(value = mean(value)*100, .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA',
                            'p1', 'p2', 'pJ', 'pA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') %>%
  transform(length = 10)



```


```{r results bias prepop, fig.width = 8, fig.height = 6}

bias.files <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'bias', all.files = FALSE, full.names = F)

rel.bias.20 <- data.frame()
for (f in 1:length(bias.files)) {
temp.bias <- data.frame(readRDS(file = here::here('results', 'simpleAge', 'sims', 'prePop',
                                                  bias.files[f])))
#use gsub on bias files rather than use different list to make sure they match
rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge', 'sims', 'prePop',
                                                     gsub('bias', 'median', bias.files[f])))[,'max_rhat'])

temp.bias$rhat <- as.numeric(rhats$rhat)
temp <- temp.bias %>% 
  reshape2::melt(id.vars = 'rhat') %>%
  transform(scenario = parse_number(gsub('S.', '', bias.files[f]))) %>%
  transform(value = ifelse(is.nan(value), 0, value))

rel.bias.20 <- rbind(rel.bias.20, temp)

} #rds file

rel.bias.20 <- rel.bias.20 %>%
  filter(rhat < 1.1) #%>%
  # transform(length = 20)

mean.rel.bias.20 <- data.frame(rel.bias.20) %>%
  group_by(variable, scenario) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(variable, detection, N, survey_freq) %>%
  dplyr::summarize(value = mean(value)*100, .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA',
                            'p1', 'p2', 'pJ', 'pA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', 
            samp_size = 'Sample size') %>%
  transform(length = 20) %>%
  bind_rows(mean.rel.bias)


#phi
plot_dat <- mean.rel.bias.20 %>% filter(grepl('phi', variable)) %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'), 
                              labels = phi_pars))

#need to add "length" metric" or filter and do dot plot below
ggplot(plot_dat, 
       aes(x = factor(detection), y = variable, fill = value)) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(N ~ survey_freq + length, drop = T, 
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
  scale_fill_gradient2(name = "Relative bias (%)",
                      mid = "#FFFFFF", low = "deepskyblue1", high = "red2", midpoint = 0) +
  theme_bw() +
  theme(legend.position = 'right',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
        legend.text = element_text(size = 8, vjust = 0.5),
        legend.title = element_text(vjust = 0.8, size = 10)) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable))))))

#p
# plot_dat <- mean.rel.bias %>% filter(variable %in% c('p1', 'p2', 'pJ', 'pA')) %>%
#   transform(variable = factor(variable, 
#                               levels = c('p1', 'p2', 'pJ', 'pA'), 
#                               labels = p_pars))
# 
# ggplot(plot_dat, aes(x = factor(detection), y = variable, fill = value)) +
#   geom_tile(color = 'grey50') +
#   xlab('Detection') +
#   ylab('Parameter') +
#   facet_nested(samp_size + N ~ survey_frequency + survey_freq, drop = T, 
#              labeller = labeller(survey_freq = freq_labels),
#              scales = 'free_x') +
#   scale_fill_gradient2(name = "Relative bias (%)",
#                       mid = "#FFFFFF", low = "deepskyblue1", high = "red2", midpoint = 0) +
#   theme_bw() +
#   theme(legend.position = 'right',
#              strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
#         legend.text = element_text(size = 8, vjust = 0.5),
#         legend.title = element_text(vjust = 0.8, size = 10)) +
#     scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable))))))

dotplot_dat <- rel.bias.20 %>% transform(length = 20) %>% 
  bind_rows(rel.bias %>% transform(length = 10)) %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(length, variable, detection, survey_freq, N) %>%
  # dplyr::summarize(value = mean(value),
  #                  lower = quantile(value, probs = 0.025),
  #                  upper = quantile(value, probs = 0.975),
    dplyr::summarize(value = mean(value)*100, .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency', samp_size = 'Sample size') %>%
  #filter out detection params
  filter(!is.na(variable))


ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & variable == 'phiP'),
       aes(x = detection, y = value, col = factor(length), group = length)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Detection') + ylab('Relative bias') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +

  facet_nested(variable + samp_size + N ~ survey_frequency + survey_freq, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +

  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2[c(5,2)], name = 'Years')

ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & length == 20 & variable %in% c('phiP')),
       aes(x = factor(survey_freq), y = value, col = factor(variable), group = variable)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding') + ylab('Relative bias') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +

  facet_nested(detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +

  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2, name = 'Rate')


```


#### RMSE  
RMSE is a combined measure of bias and variance and is generally very low. Model performance in terms of RMSE follows an expected pattern with lowest RMSE in scenarios with annual branding frequency and higher cohort sizes, and particularly the 20-year study horizon. RMSE values are generally higher for age-2 and younger due to the higher variability simulated for their survival rates by design. 


```{r results rmse}

rmse.files <- list.files(path = here::here('results', 'simpleAge', 'sims'), 
                    pattern = 'rmse', all.files = FALSE, full.names = F)

rmse <- data.frame()
for (f in 1:length(rmse.files)) {
temp.rmse <- data.frame(readRDS(file = here::here('results', 'simpleAge', 'sims',
                                                  rmse.files[f])))
#use gsub on rmse files rather than use different list to make sure they match
rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge', 'sims', 
                                                     gsub('rmse', 'median', rmse.files[f])))[,'max_rhat'])

temp.rmse$rhat <- as.numeric(rhats$rhat)
temp <- temp.rmse %>% 
  reshape2::melt(id.vars = 'rhat') %>%
  transform(scenario = parse_number(gsub('S.', '', rmse.files[f]))) %>%
  transform(value = ifelse(is.nan(value), 0, value))

rmse <- rbind(rmse, temp)

} #rds file

rmse <- rmse %>%
  filter(rhat < 1.1) #%>%
  # transform(length = 10)

mean.rmse <- data.frame(rmse) %>%
  group_by(variable, scenario) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(variable, detection, N, survey_freq) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA',
                            'p1', 'p2', 'pJ', 'pA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') %>%
  transform(length = 10) 



```


```{r results rmse prepop, fig.width = 8, fig.height = 6}

rmse.files <- list.files(path = here::here('results', 'simpleAge', 'sims', 'prePop'), 
                    pattern = 'rmse', all.files = FALSE, full.names = F)

rmse.20 <- data.frame()
for (f in 1:length(rmse.files)) {
temp.rmse <- data.frame(readRDS(file = here::here('results', 'simpleAge', 'sims', 'prePop',
                                                  rmse.files[f])))
#use gsub on rmse files rather than use different list to make sure they match
rhats <- data.frame(rhat = readRDS(file = here::here('results', 'simpleAge', 'sims', 'prePop',
                                                     gsub('rmse', 'median', rmse.files[f])))[,'max_rhat'])

temp.rmse$rhat <- as.numeric(rhats$rhat)
temp <- temp.rmse %>% 
  reshape2::melt(id.vars = 'rhat') %>%
  transform(scenario = parse_number(gsub('S.', '', rmse.files[f]))) %>%
  transform(value = ifelse(is.nan(value), 0, value))

rmse.20 <- rbind(rmse.20, temp)

} #rds file

rmse.20 <- rmse.20 %>%
  filter(rhat < 1.1) #%>%
  # transform(length = 20) 

mean.rmse.20 <- data.frame(rmse.20) %>%
  group_by(variable, scenario) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(variable, detection, N, survey_freq) %>%
  dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA',
                            'p1', 'p2', 'pJ', 'pA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency & study length', samp_size = 'Sample size') %>%
  transform(length = 20) %>%
  bind_rows(mean.rmse)


#phi
plot_dat <- mean.rmse.20 %>% filter(grepl('phi', variable)) %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'), 
                              labels = phi_pars))

ggplot(plot_dat, aes(x = factor(detection), y = variable, fill = value)) +
  geom_tile(color = 'grey50') +
  xlab('Detection') +
  ylab('Parameter') +
  facet_nested(samp_size + N ~ survey_frequency + survey_freq + length, drop = T, 
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +
 scale_fill_gradient2(name = "RMSE",
                      mid = "#FFFFFF", low = "#012345", high = "#012345", midpoint = 0) +
  theme_bw() +
  theme(legend.position = 'right',
             strip.background = element_rect(fill = "#EEEEEE", color = "#FFFFFF"),
        legend.text = element_text(size = 8, vjust = 0.5),
        legend.title = element_text(vjust = 0.8, size = 10)) +
    scale_y_discrete("", labels = parse(text = c(eval(expression(levels(plot_dat$variable))))))

# #dot plots
dotplot_dat <- rmse.20 %>% transform(length = 20) %>% 
  bind_rows(rmse %>% transform(length = 10)) %>%
  merge(scenarios, by = 'scenario', all = F) %>%
  group_by(length, variable, detection, survey_freq, N) %>%
  # dplyr::summarize(value = mean(value),
  #                  lower = quantile(value, probs = 0.025),
  #                  upper = quantile(value, probs = 0.975),
    dplyr::summarize(value = mean(value), .groups = 'keep') %>%
  transform(variable = factor(variable, levels = c('phiP', 'phi1', 'phi2', 'phiJ', 'phiA'))) %>%
  transform(detection = factor(detection, levels = c('Low', 'Med', 'High'))) %>%
  transform(survey_frequency = 'Branding frequency', samp_size = 'Sample size') %>%
  #filter out detection params
  filter(!is.na(variable))


ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & variable == 'phiP'),
       aes(x = detection, y = value, col = factor(length), group = length)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Detection') + ylab('Relative bias') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +

  facet_nested(variable + samp_size + N ~ survey_frequency + survey_freq, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +

  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2[c(5,2)], name = 'Years')

ggplot(dotplot_dat %>% filter(N %in% c(50, 100, 200) & length == 20 & variable %in% c('phiP')),
       aes(x = factor(survey_freq), y = value, col = factor(variable), group = variable)) +
  geom_point() + geom_line() +
  geom_hline(aes(yintercept = 0), linetype = 'dotted') +
  xlab('Branding') + ylab('Relative bias') +
    # facet_grid(variable~adult_p, labeller = labeller(variable = 'label_parsed'), scales = 'free') +
  # facet_grid(variable + N ~ survey_freq, label = 'label_parsed', scales = 'free') +

  facet_nested(detection ~ samp_size + N, drop = T,
             labeller = labeller(survey_freq = freq_labels, length = occ_labels),
             scales = 'free_x') +

  theme_bw() +
  plot_theme(legend.position = 'top',
             plot.subtitle = element_text(size = 10, hjust = 0.5, vjust = 1)) +
  scale_color_manual(values = rainbow2, name = 'Rate')


```


